\section{Problem Description}

The project consists on applying three different techniques on matrix multiplication and analyzing the impact of each one in the CPU performance. As such, the project was divided into three parts:

\begin{enumerate}
    \item Download the file containing the standard matrix multiplication in C++; Implement the same algorithm in a different language (we chose Java); Register processing times in both languages for matrixes from 600x600 to 3000x3000 in increments of 400.
    \item Implement in both languages a version of the algorithm using line multiplication; Repeat similar tests for this version of the algorithm; Register processing times in C++ for matrixes from 4096x4096 to 10240x10240 in increments of 2048.
    \item Implement a C++ a version of the algorithms using block multiplication; Register processing times in C++ for matrixes from 4096x4096 to 10240x10240 in increments of 2048.
\end{enumerate}

In all stages of development, the performance of the programs was also analyzed directly by requesting data from the processor on its execution via PAPI.

\section{Algorithms Explanation}

\subsection{Normal Matrix Multiplication}
The algorithm to use in the first part of the project (and that is already given) multiplies two matrixes represented as arrays of arrays (or simply arrays, depending on the implementation). In this project, only square matrixes will be used in the algorithms. 

The algorithm is composed of three nested for loops. 
For the multiplication of matrix A with matrix B and result C ($C = A \times B$):

\begin{itemize}
    \item The first for loop's variant represents A's and C's line
    \item The second for loop's variant represents B's and C's column
    \item The third for loop's variant represents B's line and A's column
\end{itemize}

\paragraph{Complexity Analysis}
\begin{itemize}
    \item \textbf{Time Complexity:} $O(N^3)$, where N is the line size of the matrixes
    \item \textbf{Space Complexity:} $O(N^2)$, where N is the line size of the matrixes
\end{itemize}

\subsection{Multiline Matrix Multiplication}
\textbf{Multiline Multiplication} is the algorithm used in the second phase of the project. This algorithm presents itself as an improvement of the previous one in terms of efficiency.

The algorihm is very similar to the previous one, with a slight change in what the loops' variants refer to. For the multiplication of matrix A with matrix B and result C ($C = A \times B$):

\begin{itemize}
    \item The first for loop's variant represents A's and C's line
    \item The second for loop's variant represents B's line and A's column
    \item The third for loop's variant represents B's and C's column
\end{itemize}

\paragraph{Complexity Analysis}
\begin{itemize}
    \item \textbf{Time Complexity:} $O(N^3)$, where N is the line size of the matrixes
    \item \textbf{Space Complexity:} $O(N^2)$, where N is the line size of the matrixes
\end{itemize}

Although the time complexity remains the same, this algorithm's efficiency is significantly greater than the previous one due to the change in the loop's variants. 

\paragraph{Explanation:} In the previous algorithm, for every iteration of the most inner loop, a cache miss will be issued by the CPU and a different row of B matrix will be loaded into the processor's cache. This happens because the matrixes are represented as an array of lines rather than columns, which means every time the processor needs an element that is not present in the line currently loaded, it will have to get the one it belongs to from main memory. As for each iteration of the most inner loop the element from B used is always in a different line, the CPU loads a new line from main memory $N^3$ times.

The \textbf{Multiline Matrix Multiplication} algorithm reduces the number of cache misses by changing the order of the loops. With the new structure, the most inner loop's variant is only used in columns rather than lines, which means only for the other two loops' iterations will the processor need to load new lines. The number of cache misses reduces to $N^2$, thus increasing the efficiency of the process.

\paragraph{Note} Cache miss means the variable requested is not present in cache memory


\subsection{Block Matrix Multiplication}
In the third part of this project, \textbf{Block Matrix Multiplication} will be used. This algorithm implements the idea of splitting a matrix into smaller matrixes and multiplying them instead. 

This algorithm is composed of 6 nested for loops. The three outer loops account for the submatrixes chosen for the calculation. The three inner loops execute \textbf{Multiline Multiplication} between the submatrixes chosen.

For the multiplication of matrix A with matrix B and result C ($C = A \times B$):

\begin{itemize}
    \item \textbf{3 Outer Loops:}
    \begin{itemize}
        \item The first outer for loop's variant represents the fourth outer loop (first inner loop)'s variant limits, which are the lines from A and C to be selected to the submatrix A and submatrix C
        \item The second outer for loop's variant represents the fifth outer loop (second inner loop)'s variant limits, which are the columns from A and lines from B to be selected to the submatrix A and submatrix B
        \item The third outer for loop's variant represents the sixth outer loop (third inner loop)'s variant limits, which are the columns from B and C to be selected to the submatrix B and submatrix C
    \end{itemize}
    \item \textbf{3 Inner Loops:}
    \begin{itemize}
        \item The first for loop's variant represents A's and C's line
        \item The second for loop's variant represents B's line and A's column
        \item The third for loop's variant represents B's and C's column
    \end{itemize}
\end{itemize}


\paragraph{Complexity Analysis}
\begin{itemize}
    \item \textbf{Time Complexity:} $O(N^3 \div B^3 \times B^3 = N^3)$, where N is the line size of the matrixes
    \item \textbf{Space Complexity:} $O(N^2)$, where N is the line size of the matrixes
\end{itemize}

Although the time complexity is the same, once again this algorithm is a big improvement from the previous one, specially in multiplication of larger matrixes.

\paragraph{Explanation:} This algorithm, once again, is more efficient due to the optimization related with the number of cache misses. Some matrixes are so big that a whole line can't fit in the processor's cache. By splitting the matrix into smaller chunks that fit in it, the number of cache misses can be drastically reduced.
